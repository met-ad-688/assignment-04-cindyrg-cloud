---
title: Assignment 04
author:
  - name: Norah Jones
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
number-sections: true
date: '2024-11-21'
date-modified: today
date-format: long
format:
  html:
    theme: cerulean
    toc: true
    toc-depth: 2

execute:
  echo: false
  eval: false
  freeze: auto
---

```{python}
# Set Up
from pyspark.sql import SparkSession
import pandas as pd
import plotly.express as px
import plotly.io as pio
import numpy as np
from pathlib import Path

# Use a renderer that works during Quarto render
pio.renderers.default = "plotly_mimetype"

# Start a Spark session
spark = SparkSession.builder.appName("LightcastData").getOrCreate()

np.random.seed(42)

import sys, subprocess
subprocess.run([sys.executable, "-m", "pip", "install", "seaborn", "matplotlib"], check=False)

print("[OK] Setup complete.")   
```

```{python}

from pathlib import Path

# Path relative to this document's working directory
data_path = Path("data") / "lightcast_job_postings.csv"

# Load the CSV file into a Spark DataFrame
df = (
spark.read
.option("header", "true")
.option("inferSchema", "true")
.option("multiLine","true")
.option("escape", "\"")
.csv("./data/lightcast_job_postings.csv"))

# df.printSchema() # comment this line when rendering the submission
df.show(5)
```


```{python}
# Variable Selection
y_col = "SALARY"
min_years = "MIN_YEARS_EXPERIENCE"
cont_cols = ["MAX_YEARS_EXPERIENCE", "DURATION", "SALARY_FROM"]
cat_cols = ["COMPANY_NAME", "LOT_V6_SPECIALIZED_OCCUPATION_NAME"]

# Data clean up -- dropping NAs 
from pyspark.sql.functions import col

required_cols = [y_col, min_years] + cont_cols + cat_cols
missing = [c for c in required_cols if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns in DataFrame: {missing}")

df_clean = df.dropna(subset=required_cols)

# Casting numeric columns to double (Tip found on Reddit)
for c in [y_col, min_years] + cont_cols:
    df_clean = df_clean.withColumn(c, col(c).cast("double"))

# Check
print("[OK] Variables set and data cleaned.")
print("[vars] continuous:", cont_cols, "| + min_years:", min_years)
print("[vars] categorical:", cat_cols)

```


```{python}
# Encoders for categories
from pyspark.ml.feature import StringIndexer, OneHotEncoder, SQLTransformer, VectorAssembler
from pyspark.ml import Pipeline

indexers = [
    StringIndexer(
        inputCol=c,
        outputCol=f"{c}_idx",
        handleInvalid="keep",           # keep unseen/invalid as a special bucket
        stringOrderType="frequencyDesc" 
    )
    for c in cat_cols
]

encoder = OneHotEncoder(
    inputCols=[f"{c}_idx" for c in cat_cols],
    outputCols=[f"{c}_ohe" for c in cat_cols],
    handleInvalid="keep"
)


add_square = SQLTransformer(
    statement=f"SELECT *, POW({min_years}, 2.0) AS {min_years}_SQ FROM __THIS__"
)

# Ensure numeric label for ML
add_label = SQLTransformer(
    statement=f"SELECT *, CAST({y_col} AS DOUBLE) AS label FROM __THIS__"
)

# Assemble base & polynomial features
from pyspark.ml.feature import VectorAssembler

# features = 3 continuous + MIN_YEARS_EXPERIENCE + 2 OHE categoricals
feature_cols = cont_cols + [min_years] + [f"{c}_ohe" for c in cat_cols]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# polynomial features from MIN_YEARS_EXPERIENCE only
poly_cols = [min_years, f"{min_years}_SQ"]
poly_assembler = VectorAssembler(inputCols=poly_cols, outputCol="features_poly")

pipeline = Pipeline(stages=indexers + [encoder, add_square, add_label, assembler, poly_assembler])

# Fit & transform
model = pipeline.fit(df_clean)
final_df = model.transform(df_clean)

print("[OK] Pipeline fitted and data transformed.")
final_df.select("label", min_years, f"{min_years}_SQ", "features", "features_poly").show(5, truncate=False)
```

```{python}
# 5) Add a stable row id and split
from pyspark.sql.functions import monotonically_increasing_id

final_df = final_df.withColumn("row_id", monotonically_increasing_id())
train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)

```

```{python}
# === GLR with robust coefficient table (with bootstrap fallback) ===
from pyspark.ml.regression import GeneralizedLinearRegression
from pyspark.sql import functions as F
from pyspark.ml.feature import OneHotEncoderModel
import numpy as np, pandas as pd
from math import erf, sqrt

# 1) Fit GLR (Gaussian/Identity) on train
glr = GeneralizedLinearRegression(
    featuresCol="features", labelCol="label",
    family="gaussian", link="identity", maxIter=50
)
glr_model = glr.fit(train_df)
glr_summary = glr_model.summary

# 2) Test metrics
glr_test = glr_model.transform(test_df)
glr_rmse = glr_test.select(F.sqrt(F.avg((F.col("label")-F.col("prediction"))**2)).alias("rmse")).first()["rmse"]
glr_mae  = glr_test.select(F.avg(F.abs(F.col("label")-F.col("prediction"))).alias("mae")).first()["mae"]

print("=== GLR (Gaussian/Identity) Results ===")
print(f"Intercept: {glr_model.intercept:.4f}")
print(f"AIC (train): {glr_summary.aic:.4f}")
print(f"Deviance (train): {glr_summary.deviance:.4f} | Dispersion: {glr_summary.dispersion:.4f}")
print(f"RMSE (test): {glr_rmse:.4f} | MAE (test): {glr_mae:.4f}")

# 3) Build expanded feature names that exactly match the coefficients length
enc_model = next(s for s in model.stages if isinstance(s, OneHotEncoderModel))

# account for dropLast behavior of the fitted OHE model
drop_last = enc_model.getDropLast() if hasattr(enc_model, "getDropLast") else True

expanded_features = cont_cols + [min_years]
for input_col, size_in in zip(enc_model.getInputCols(), enc_model.categorySizes):
    base = input_col.replace("_idx", "")
    size_out = int(size_in) - (1 if drop_last else 0)
    expanded_features += [f"{base}_ohe_{i}" for i in range(size_out)]

coefs = np.array(glr_model.coefficients.toArray(), dtype=float)
print(f"[debug] coeffs: {len(coefs)} | feature names: {len(expanded_features)}")

if len(coefs) != len(expanded_features):
    # last resort: truncate/pad names to match (shouldn't happen, but keeps notebook running)
    if len(expanded_features) > len(coefs):
        expanded_features = expanded_features[:len(coefs)]
    else:
        expanded_features += [f"extra_{i}" for i in range(len(coefs)-len(expanded_features))]

# 4) Try to pull SE/t/p; if Spark won’t provide them, estimate via bootstrap
stderr = t_vals = p_vals = None
try:
    stderr  = np.array(glr_summary.coefficientStandardErrors, dtype=float)
    t_vals  = np.array(glr_summary.tValues, dtype=float)
    p_vals  = np.array(glr_summary.pValues, dtype=float)
    print("[info] Using GLR-provided standard errors/t/p.")
except Exception as e:
    print(f"[warn] Spark did not provide coefficient SE/t/p ({type(e).__name__}). Falling back to bootstrap…")
    B = 40  # adjust if you want faster (smaller) or more stable (larger)
    boot_coefs = []

    for b in range(B):
        boot = train_df.sample(withReplacement=True, fraction=1.0, seed=1234 + b)
        boot_model = GeneralizedLinearRegression(
            featuresCol="features", labelCol="label",
            family="gaussian", link="identity", maxIter=50
        ).fit(boot)
        c = np.array(boot_model.coefficients.toArray(), dtype=float)
        # align length defensively
        if len(c) == len(coefs):
            boot_coefs.append(c)

    boot_coefs = np.array(boot_coefs)
    if boot_coefs.shape[0] < 3:
        print("[warn] Not enough successful bootstrap fits to estimate SE. Reporting coefficients only.")
        stderr = np.full_like(coefs, np.nan)
        t_vals = np.full_like(coefs, np.nan)
        p_vals = np.full_like(coefs, np.nan)
    else:
        stderr = boot_coefs.std(axis=0, ddof=1)
        # Normal-approx t and p
        with np.errstate(divide='ignore', invalid='ignore'):
            t_vals = coefs / stderr
            # two-sided p using normal approx (no SciPy needed)
            def norm_cdf(z): return 0.5 * (1.0 + erf(z / sqrt(2.0)))
            p_vals = 2.0 * (1.0 - np.vectorize(norm_cdf)(np.abs(t_vals)))

# 5) Build coefficient table with 95% CI (if SE available)
coef_pd = pd.DataFrame({
    "feature": expanded_features,
    "coefficient": coefs,
    "stderr": stderr,
    "t_value": t_vals,
    "p_value": p_vals
})

if np.all(np.isnan(stderr)):
    coef_pd["CI_lower"] = np.nan
    coef_pd["CI_upper"] = np.nan
else:
    coef_pd["CI_lower"] = coef_pd["coefficient"] - 1.96 * coef_pd["stderr"]
    coef_pd["CI_upper"] = coef_pd["coefficient"] + 1.96 * coef_pd["stderr"]

display(coef_pd)


```

```{python}
# 7) Linear Regression & Polynomial LR (no std errors)
from pyspark.ml.regression import LinearRegression

# Linear
lr = LinearRegression(featuresCol="features", labelCol="label")
lr_model = lr.fit(train_df)
lr_test = lr_model.transform(test_df)

# Polynomial
poly_lr = LinearRegression(featuresCol="features_poly", labelCol="label")
poly_model = poly_lr.fit(train_df)
poly_test = poly_model.transform(test_df)

```

```{python}
# 8) Random Forest + importances
from pyspark.ml.regression import RandomForestRegressor
import numpy as np, seaborn as sns, matplotlib.pyplot as plt

rf = RandomForestRegressor(
    featuresCol="features", labelCol="label",
    numTrees=200, maxDepth=6, seed=42
)
rf_model = rf.fit(train_df)
rf_test = rf_model.transform(test_df)

# Feature importances aligned to expanded_features
rf_importances = np.array(rf_model.featureImportances.toArray())
fi = pd.DataFrame({"feature": expanded_features, "importance": rf_importances})
fi = fi.sort_values("importance", ascending=False).head(10)

plt.figure(figsize=(8,5))
sns.barplot(data=fi, y="feature", x="importance")
plt.title("Random Forest — Top 10 Feature Importances")
plt.tight_layout()
Path("_output").mkdir(parents=True, exist_ok=True)
plt.savefig("_output/rf_feature_importance.png", dpi=180)
plt.show()
print("Saved: _output/rf_feature_importance.png")
```

```{python}
# 9) Unified metrics & plots (safe join on row_id, no ambiguous label)

from pyspark.sql import functions as F
import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from pyspark.ml.feature import OneHotEncoderModel

# --- Keep label only once (from GLR) ---
p_glr = glr_test.select("row_id", F.col("label").alias("label"), F.col("prediction").alias("pred_glr"))
p_lr  = lr_test.select("row_id", F.col("prediction").alias("pred_lr"))            # no label here
p_pr  = poly_test.select("row_id", F.col("prediction").alias("pred_poly"))        # no label here
p_rf  = rf_test.select("row_id", F.col("prediction").alias("pred_rf"))            # no label here

# Inner-join on row_id (stable key we added before the split)
joined = (
    p_glr.join(p_lr, "row_id", "inner")
         .join(p_pr, "row_id", "inner")
         .join(p_rf, "row_id", "inner")
)

# To Pandas for metrics/plots
pdf = joined.select("label","pred_glr","pred_lr","pred_poly","pred_rf").toPandas().dropna()

# --- Targets & predictions ---
y = pdf["label"].values
preds = {
    "GLR":         pdf["pred_glr"].values,
    "Linear":      pdf["pred_lr"].values,
    "Polynomial":  pdf["pred_poly"].values,
    "RandomForest":pdf["pred_rf"].values
}

# --- Helper metrics ---
def rmse(y, yhat): 
    return float(np.sqrt(np.mean((y - yhat)**2)))

def aic_bic(y, yhat, k):
    n = len(y)
    sse = float(np.sum((y - yhat)**2))
    if n <= 0 or sse <= 0:
        return np.nan, np.nan
    sigma2 = sse / n
    # Gaussian OLS log-likelihood
    logL = -0.5 * n * (np.log(2*np.pi*sigma2) + 1.0)
    AIC = 2*k - 2*logL
    BIC = k*np.log(n) - 2*logL
    return float(AIC), float(BIC)

# --- Parameter counts ---
# GLR/LR/Poly: #coeffs + intercept
k_glr  = len(glr_model.coefficients) + 1
k_lr   = len(lr_model.coefficients) + 1
k_poly = len(poly_model.coefficients) + 1

# RF proxy: width of features vector (+ intercept)
# Recompute expanded f

```

```{python}
# Optional: Plotly horizontal bar for GLR coefficients
import plotly.express as px
coef_plot = coef_pd.sort_values("coefficient")
fig = px.bar(coef_plot, x="coefficient", y="feature", orientation="h",
             title="GLR Coefficients with 95% CI",
             error_x=coef_plot["CI_upper"] - coef_plot["coefficient"],
             error_x_minus=coef_plot["coefficient"] - coef_plot["CI_lower"])
fig.show()
```



